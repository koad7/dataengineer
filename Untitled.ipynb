{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy.sparse as sparse\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing\n",
    "import sys, os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor,as_completed\n",
    "from functools import partial\n",
    "import urllib.request\n",
    "import fiona\n",
    "import rasterio.mask\n",
    "import rasterio\n",
    "import requests\n",
    "import gzip\n",
    "import pprint\n",
    "import logging\n",
    "import click\n",
    "import itertools\n",
    "import glob\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "logging.basicConfig(filename='app.log', filemode='w', format='%(asctime)s — %(name)s — %(levelname)s — %(funcName)s:%(lineno)d — %(message)s',level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_downloaded_files(folderpath):\n",
    "    '''\n",
    "    Delete all files in given folder. Main usae is to erase all downloaded files\n",
    "    folderpath: folder path\n",
    "    Parameters\n",
    "    ----------\n",
    "    folderpath : Path to the directory to erase files from\n",
    "    Returns\n",
    "    -------\n",
    "    '''\n",
    "    files = glob.glob(folderpath+'*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "        \n",
    "\n",
    "def proj_init():\n",
    "    '''\n",
    "    Init function for folders creation and determining useful number of threads to use\n",
    "    Parameters\n",
    "    ----------\n",
    "    ''\n",
    "    Returns\n",
    "    -------\n",
    "    DOWNLOADS_DIR: Folder where the CHIRPS files are downloaded in \n",
    "    MASKED_FILES_DIR: Folder where the masked files are saved in\n",
    "    SATCKED_FILES_DIR: Folder where the stacked files are saved in\n",
    "    THREADS: Number of thread that the multithreading functions will use\n",
    "    '''\n",
    "    # CREATE WORK DIRS \n",
    "    DOWNLOADS_DIR='/tmp/downloads/' # downloaded files\n",
    "    if not os.path.exists(DOWNLOADS_DIR):\n",
    "        os.makedirs(DOWNLOADS_DIR)\n",
    "    MASKED_FILES_DIR='/tmp/masked/' # created masked files\n",
    "    if not os.path.exists(MASKED_FILES_DIR): \n",
    "        os.makedirs(MASKED_FILES_DIR)\n",
    "    SATCKED_FILES_DIR='/tmp/stacked/'\n",
    "    if not os.path.exists(SATCKED_FILES_DIR): # created stacked files\n",
    "        os.makedirs(SATCKED_FILES_DIR)\n",
    "    # Nbr of thread to use for multiprocessing\n",
    "    if (multiprocessing.cpu_count() > 2):\n",
    "        THREADS=multiprocessing.cpu_count()-2\n",
    "    else:\n",
    "        THREADS=2 # At least 2 for multi threading\n",
    "    return {'DOWNLOADS_DIR':DOWNLOADS_DIR,'MASKED_FILES_DIR':MASKED_FILES_DIR, 'SATCKED_FILES_DIR':SATCKED_FILES_DIR,'THREADS':THREADS }\n",
    "\n",
    "\n",
    "def files_url_list(url,files,year):\n",
    "    '''\n",
    "    Build the files url list from the CHIRPS website. Use beautiful soup to extract urls from thewebsite html. \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: url of the website to download the data from\n",
    "    files: object to store the files url in\n",
    "    year: year of selection\n",
    "    Returns\n",
    "    -------\n",
    "    no return \n",
    "    '''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    for node in soup.find_all('a'):\n",
    "        try:\n",
    "            if(node.get('href').endswith('tif') | node.get('href').endswith('gz')):\n",
    "                if(node.get('href').split('.')[-4]==str(year) or node.get('href').split('.')[-5]==str(year)):\n",
    "                    files.append( url + '/' + node.get('href'))\n",
    "        except Exception as e:\n",
    "            logging.exception(\"files_url_list: Exception caught during processing\")\n",
    "\n",
    "\n",
    "            \n",
    "def concurrent_files_url_list(baseUrl, years):\n",
    "    '''\n",
    "    Concurrent Donwload of .tiff file urls in a list\n",
    "    Parameters\n",
    "    ----------\n",
    "    baseUrl: url of the page to download the files from\n",
    "    years: list of year(s) of selection\n",
    "    Returns\n",
    "    -------\n",
    "    files: list or urls to download\n",
    "    '''\n",
    "    files={}\n",
    "    # Concurent downloading of the data\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    append_data=[]\n",
    "    result=[]\n",
    "    # Concurences\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        for year in years:\n",
    "            files[str(year)]=[]\n",
    "            if(baseUrl.split('_')[-1].split('/')[0]=='daily'):# In case using daily rainfall data in the daily folder\n",
    "                try:\n",
    "                    append_data.append(executor.submit(files_url_list, baseUrl+str(year), files[str(year)],year))\n",
    "                except Exception as e:\n",
    "                    logging.exception(\"download_file_links: Exception caught during processing\")\n",
    "            elif(baseUrl.split('_')[-1].split('/')[0]=='monthly'):# In case using daily rainfall data in the monthly folder\n",
    "                try:\n",
    "                    append_data.append(executor.submit(files_url_list, baseUrl, files[str(year)],year))\n",
    "                except Exception as e:\n",
    "                    logging.exception(\"download_file_links: Exception caught during processing\")\n",
    "\n",
    "    # Retrieve values\n",
    "    for task in as_completed(append_data):\n",
    "        try:\n",
    "            result.append(task.result())\n",
    "        except Exception as e:\n",
    "                logging.exception(\"download_file_links: Exception caught during processing\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def download_file(url):\n",
    "    '''\n",
    "    Download the .tif or .gz files and uncompress the .gz file in memory\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: url of the file to download\n",
    "    Returns\n",
    "    -------\n",
    "    no return. Downloads files into DOWNLOADS_DIR\n",
    "    '''\n",
    "    if (url.endswith('gz')):\n",
    "        outFilePath = DOWNLOADS_DIR+url.split('/')[-1][:-3]\n",
    "    else:\n",
    "        outFilePath = DOWNLOADS_DIR+url.split('/')[-1]\n",
    "    response = urllib.request.urlopen(url) \n",
    "    with open(outFilePath, 'wb') as outfile:\n",
    "        if (url.endswith('gz')):\n",
    "            outfile.write(gzip.decompress(response.read()))\n",
    "            gzip.decompress(response.read())\n",
    "        elif (url.endswith('tif')):\n",
    "            outfile.write(response.read())\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "def concurrent_file_downloader(files):    \n",
    "    '''\n",
    "    Concurent downloading and extraction of the data\n",
    "    Parameters\n",
    "    ----------\n",
    "    files: list of url to download\n",
    "    Returns\n",
    "    -------\n",
    "    no return\n",
    "    '''\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    append_data=[]\n",
    "    result=[]\n",
    "    # Concurences\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        for year in files:\n",
    "            for url in files[year]:\n",
    "                try:\n",
    "                    append_data.append(executor.submit(download_file,url))\n",
    "                except Exception as e:\n",
    "                    logging.exception(\"concurrent_file_downloader: Exception caught during processing\")\n",
    "\n",
    "    # Retrieve values\n",
    "    for task in as_completed(append_data):\n",
    "        try:\n",
    "            result.append(task.result())\n",
    "        except Exception as e:\n",
    "            logging.exception(\"concurrent_file_downloader: Exception caught during processing\")\n",
    "\n",
    "\n",
    "def aoi_shapefile_reader(aoishapefile):\n",
    "    '''\n",
    "    Download the shapefile of the area of interest (aoi)\n",
    "    Parameters\n",
    "    ----------\n",
    "    aoishapefile: path to the shapefile to download\n",
    "    Returns\n",
    "    -------\n",
    "    shapes: file containing the coordinates of the aoi's polygon\n",
    "    '''\n",
    "    # Read the AOI's shapefile\n",
    "    with fiona.open(aoishapefile, \"r\") as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def masking(file,shapes):\n",
    "    '''Masking of .tif files by the provided shapefile\n",
    "    Parameters\n",
    "    ----------\n",
    "    file: .tif file to be masked\n",
    "    shapes: Coordinate of the aoi polygon\n",
    "    Returns\n",
    "    -------\n",
    "    export files into MASKED_FILES_DIR directory\n",
    "    '''\n",
    "    if file[-4:]=='.tif':\n",
    "        with rasterio.open(DOWNLOADS_DIR+file) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "            out_meta = src.meta\n",
    "        # use the updated spatial transform and raster height and width to write the masked raster to a new file.   \n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "        with rasterio.open(MASKED_FILES_DIR+file[:-4]+\".masked.tif\", \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "            \n",
    "\n",
    "def concurrent_masking(shapes):\n",
    "    '''\n",
    "    Launch the concurent masking of the list of .tiff files by the aio provided\n",
    "    Parameters\n",
    "    ----------\n",
    "    shapes: Coordinate of the aoi polygon\n",
    "    Returns\n",
    "    -------\n",
    "    '''\n",
    "    append_data=[]\n",
    "    result=[]\n",
    "    # Concurent masking\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "         for file in os.listdir(DOWNLOADS_DIR):\n",
    "            try:\n",
    "                append_data.append(executor.submit(masking,file,aoishapes))\n",
    "            except Exception as e:\n",
    "                logging.exception(\"concurrent_file_downloader: Exception caught during processing\")\n",
    "\n",
    "    # Retrieve values\n",
    "    for task in as_completed(append_data):\n",
    "        try:\n",
    "            result.append(task.result())\n",
    "        except Exception as e:\n",
    "            logging.exception(\"concurrent_file_downloader: Exception caught during processing\")\n",
    "            \n",
    "\n",
    "def calculate_rainy_days(years):\n",
    "    '''\n",
    "    Calculate the number of rainy dates in month over a year\n",
    "    Parameters\n",
    "    ----------\n",
    "    years: List of years selected\n",
    "    Returns\n",
    "    -------\n",
    "    OrderedDict of month - rainy days\n",
    "    '''\n",
    "    MONTHS_DICT={1:'Jan', 2: 'Feb', 3:'Mar',4:'Apr', 5:'May',6:'Jun',7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}\n",
    "    data_array=[]\n",
    "    Mat = pd.DataFrame()\n",
    "    table=pd.DataFrame(index=np.arange(0,1))\n",
    "    for file in os.listdir(MASKED_FILES_DIR):\n",
    "        if file[-4:]=='.tif':\n",
    "            dataset = rasterio.open(MASKED_FILES_DIR+file)\n",
    "            widht=dataset.profile.get('width')\n",
    "            height=dataset.profile.get('height')\n",
    "            data_array=dataset.read(1)\n",
    "            data_array_sparse=sparse.coo_matrix(data_array,shape=(height,widht))\n",
    "            if(baseUrl.split('_')[-1].split('/')[0]=='daily'):\n",
    "                data = file[12:-11]\n",
    "            elif(baseUrl.split('_')[-1].split('/')[0]=='monthly'):\n",
    "                data = file[12:-14]\n",
    "            Mat[data]=data_array_sparse.toarray().tolist()\n",
    "\n",
    "    # Calculate the precipitaion per day dataframe\n",
    "    raindatadf = pd.DataFrame(Mat.applymap(lambda x: sum([t for t in x])).sum()).T #sum the array in each dataframe cells\n",
    "    rainy_days={}\n",
    "    for year in years:\n",
    "        number_of_days={}\n",
    "        for i in range(1,13): # for 12 months\n",
    "            number_of_days[MONTHS_DICT[i]]=raindatadf[raindatadf.columns[raindatadf.columns.str.slice(0,7).str.endswith(f'{year}.{i:02}')]].gt(0.0).sum(axis=1)[0]\n",
    "        rainy_days[year]=number_of_days\n",
    "    return collections.OrderedDict(sorted(rainy_days.items())) \n",
    "\n",
    "\n",
    "def order_masked_files_per_month():\n",
    "    '''\n",
    "    Order masked .tiff files from MASKED_FILES_DIR  per month.\n",
    "    Parameters\n",
    "    ----------\n",
    "    years: List of years selected\n",
    "    Returns\n",
    "    -------\n",
    "    rasterfiles: dictionnary of list of masked .tiff files ordered by month\n",
    "    '''\n",
    "    rasterfiles={}\n",
    "    key = lambda x: x[17:19] # Month\n",
    "    masked_raster=os.listdir(MASKED_FILES_DIR)\n",
    "    masked_raster=sorted(masked_raster, key=key)\n",
    "    myfileslist=[]\n",
    "\n",
    "    for key, group in itertools.groupby(masked_raster, key):\n",
    "        myfileslist.append(list(group))\n",
    "    # Month selection as key\n",
    "    for l in myfileslist:\n",
    "        rasterfiles[str(l[0][17:19])]=l \n",
    "    return rasterfiles\n",
    "\n",
    "\n",
    "def stack_rasters(filelist,month):\n",
    "    '''\n",
    "    Stack list of .tif files by month\n",
    "    Parameters\n",
    "    ----------\n",
    "    filelist: List of masked tiff images\n",
    "    month: month selected in number ('01', '02'...'12')\n",
    "    Returns\n",
    "    -------\n",
    "    no return. Copy the stacked .tiff images in SATCKED_FILES_DIR\n",
    "    '''\n",
    "    with rasterio.open(MASKED_FILES_DIR+filelist[0]) as src0:\n",
    "        meta = src0.meta\n",
    "\n",
    "    # Update meta to reflect the number of layers\n",
    "    meta.update(count = len(filelist))\n",
    "\n",
    "    # Read each layer and write it to stack\n",
    "    yearslist='_'.join(map(str, [years[-1], years[0]]))\n",
    "    with rasterio.open(f'{SATCKED_FILES_DIR}stacked_{yearslist}.{month}.tif', 'w', **meta) as dst:\n",
    "        for id, layer in enumerate(filelist, start=1):\n",
    "            with rasterio.open(MASKED_FILES_DIR+layer) as src1:\n",
    "                dst.write_band(id, src1.read(1))\n",
    "\n",
    "\n",
    "                    \n",
    "def concurrent_stack_rasters():\n",
    "    '''\n",
    "    Launch the concurent stacking of the tiff images\n",
    "    Parameters\n",
    "    ----------\n",
    "    No parameters\n",
    "    Returns\n",
    "    -------\n",
    "    No return. Copy the stacked .tiff images in SATCKED_FILES_DIR\n",
    "    '''\n",
    "    appenddata=[]\n",
    "    result=[]\n",
    "    # Concurent masking\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "         for month in rasterfiles:\n",
    "            try:\n",
    "                #int(month[:3]) # trick to remove .DS_STORE file\n",
    "                filelist=rasterfiles[month] \n",
    "                pp.pprint(month)\n",
    "                appenddata.append(executor.submit(stack_rasters,filelist,month))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    # Retrieve values\n",
    "    for task in as_completed(appenddata):\n",
    "        try:\n",
    "            result.append(task.result())\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Init and folder creation. Threads number determination\n",
    "    myenv=proj_init()\n",
    "    DOWNLOADS_DIR=myenv['DOWNLOADS_DIR']\n",
    "    MASKED_FILES_DIR=myenv['MASKED_FILES_DIR']\n",
    "    SATCKED_FILES_DIR=myenv['SATCKED_FILES_DIR']\n",
    "    THREADS=myenv['THREADS']\n",
    "    # Selection of the dataset base URL \n",
    "    BASE_URL='https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/tifs/p25/'\n",
    "    # Selection of the year(s) of interest\n",
    "    YEARS=[ 2020]#, 2014, 2013, 2012, 2011, 2010]\n",
    "    # get all files ulrs from the CHIRPS dataset web page\n",
    "    files=concurrent_files_url_list(BASE_URL, YEARS)\n",
    "    # launch concurent dowload of all the .tiff files\n",
    "    concurrent_file_downloader(files)\n",
    "    # dowload aoi\n",
    "    aoishapes=aoi_shapefile_reader(\"data/aoi.shp\")\n",
    "    # Masking\n",
    "    concurrent_masking(aoishapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
